{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finetuning + Distillation + Quantization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> <b>Data Perparation</b> </h4> \n",
    "<h5> Data Sources </h5>\n",
    "<ol>\n",
    "  <li> <a href=\"https://www.kaggle.com/datasets/sunilthite/llm-detect-ai-generated-text-dataset\"> LLM - Detect AI Generated Text Dataset </a> </li>\n",
    "  <li> <a href=\"https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset\"> DAIGT V2 Train Dataset </a> </li>\n",
    "</ol>\n",
    "\n",
    "<p>Both of these CSV files were added to the following kaggle notebook as an input for merging them and converting them to get our desired output format.</p>\n",
    "<a href=\"https://www.kaggle.com/code/openmihirpatel/aivsog-dataprep\"> Kaggle Notebook for data preparation and preprocessing. </a>\n",
    "<p>After follwoing the notebook a new combined csv file will be generated (in the output folder) which will be our combined training data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Fine tuning and Distillation</b></h3>\n",
    "<h5><b>Fine Tuning</b></h5><p>We will use <b>BERT-base</b> available on <a href=\"https://huggingface.co/google-bert/bert-base-uncased\"> Hugging Face </a> as our teacher model after finetuning it on our dowstream task (which is Text-Classification).</p>\n",
    "\n",
    "<h5><b>Distillation</b></h5><p>We will use <b>DistilBERT-base</b> available on <a href=\"https://huggingface.co/distilbert/distilbert-base-uncased\"> Hugging Face </a> as our student model for distillation with the help of our teacher model.</p>\n",
    "\n",
    "<p>The following notebook will take us to model distillation and quantization part of this project.</p>\n",
    "<p><b><a href=\"https://www.kaggle.com/code/openmihirpatel/finetuning-and-distillation\">Model Distillation and Quantization</a></b></p>\n",
    "\n",
    "<h5>Results discussion</h5>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Model type</th>\n",
    "    <th>Accuracy</th>\n",
    "    <th>Loss</th>\n",
    "    <th>Params</th>\n",
    "    <th>Size (after compression)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BERT-base-uncased (Finetuned)</td>\n",
    "    <td> 0.9919028340080972 </td>\n",
    "    <td> 0.03693790721925015 </td>\n",
    "    <td> 109.48 M </td>\n",
    "    <td> 438.02 MB </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Distilled-BERT-base-uncased (Distilled)</td>\n",
    "    <td> 0.9892037786774629 </td>\n",
    "    <td> 0.05357428242828935 </td>\n",
    "    <td> 66.36 M </td>\n",
    "    <td> 265.5 MB </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
